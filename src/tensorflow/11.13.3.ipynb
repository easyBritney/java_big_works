{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chars\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fcn_layer(inputs,\n",
    "             input_dim,\n",
    "             output_dim,\n",
    "             activation=None):\n",
    "    W = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros([output_dim]))\n",
    "    XWb = tf.matmul(inputs, W) + b\n",
    "    if activation is None:\n",
    "        outputs = XWb\n",
    "    else:\n",
    "        outputs = activation(XWb)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784], name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1 = fcn_layer(inputs=x,\n",
    "              input_dim=784,\n",
    "              output_dim=256,\n",
    "              activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forward = fcn_layer(inputs=h1,\n",
    "                   input_dim=256,\n",
    "                   output_dim=10)\n",
    "pred = tf.nn.softmax(forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, [None, 10], name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=forward, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_epochs = 40\n",
    "batch_size = 50\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "display_step = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(pred, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 01 Loss= 0.131486014  Accurary= 0.9632\n",
      "Train Epoch: 02 Loss= 0.092951484  Accurary= 0.9722\n",
      "Train Epoch: 03 Loss= 0.072501749  Accurary= 0.9778\n",
      "Train Epoch: 04 Loss= 0.068726808  Accurary= 0.9802\n",
      "Train Epoch: 05 Loss= 0.067405887  Accurary= 0.9786\n",
      "Train Epoch: 06 Loss= 0.063583292  Accurary= 0.9832\n",
      "Train Epoch: 07 Loss= 0.065947711  Accurary= 0.9798\n",
      "Train Epoch: 08 Loss= 0.069386363  Accurary= 0.9812\n",
      "Train Epoch: 09 Loss= 0.065605320  Accurary= 0.9814\n",
      "Train Epoch: 10 Loss= 0.066608243  Accurary= 0.9828\n",
      "Train Epoch: 11 Loss= 0.074201092  Accurary= 0.9812\n",
      "Train Epoch: 12 Loss= 0.070901975  Accurary= 0.9816\n",
      "Train Epoch: 13 Loss= 0.075154550  Accurary= 0.9802\n",
      "Train Epoch: 14 Loss= 0.076303124  Accurary= 0.9818\n",
      "Train Epoch: 15 Loss= 0.089137547  Accurary= 0.9792\n",
      "Train Epoch: 16 Loss= 0.089077443  Accurary= 0.9818\n",
      "Train Epoch: 17 Loss= 0.079449669  Accurary= 0.9818\n",
      "Train Epoch: 18 Loss= 0.080138817  Accurary= 0.9838\n",
      "Train Epoch: 19 Loss= 0.073526800  Accurary= 0.9848\n",
      "Train Epoch: 20 Loss= 0.078066714  Accurary= 0.9814\n",
      "Train Epoch: 21 Loss= 0.088028364  Accurary= 0.9814\n",
      "Train Epoch: 22 Loss= 0.082960673  Accurary= 0.9830\n",
      "Train Epoch: 23 Loss= 0.099615425  Accurary= 0.9808\n",
      "Train Epoch: 24 Loss= 0.100955479  Accurary= 0.9796\n",
      "Train Epoch: 25 Loss= 0.097119138  Accurary= 0.9828\n",
      "Train Epoch: 26 Loss= 0.103489026  Accurary= 0.9798\n",
      "Train Epoch: 27 Loss= 0.109041855  Accurary= 0.9782\n",
      "Train Epoch: 28 Loss= 0.107283950  Accurary= 0.9820\n",
      "Train Epoch: 29 Loss= 0.105025649  Accurary= 0.9802\n",
      "Train Epoch: 30 Loss= 0.114229098  Accurary= 0.9808\n",
      "Train Epoch: 31 Loss= 0.095804356  Accurary= 0.9828\n",
      "Train Epoch: 32 Loss= 0.100202829  Accurary= 0.9830\n",
      "Train Epoch: 33 Loss= 0.111323826  Accurary= 0.9808\n",
      "Train Epoch: 34 Loss= 0.095554151  Accurary= 0.9840\n",
      "Train Epoch: 35 Loss= 0.117546290  Accurary= 0.9824\n",
      "Train Epoch: 36 Loss= 0.107733913  Accurary= 0.9840\n",
      "Train Epoch: 37 Loss= 0.107487440  Accurary= 0.9824\n",
      "Train Epoch: 38 Loss= 0.118745558  Accurary= 0.9816\n",
      "Train Epoch: 39 Loss= 0.114044227  Accurary= 0.9800\n",
      "Train Epoch: 40 Loss= 0.128952831  Accurary= 0.9802\n",
      "Train Finished takes: 110.97\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "startTime = time()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    for batch in range(total_batch):\n",
    "        xs, ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={x: xs, y: ys})\n",
    "        \n",
    "    loss, acc = sess.run([loss_function, accuracy],\n",
    "                        feed_dict={x: mnist.validation.images,\n",
    "                                  y: mnist.validation.labels})\n",
    "    \n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print(\"Train Epoch:\", '%02d' % (epoch + 1),\n",
    "             \"Loss=\", \"{:.9f}\".format(loss), \" Accurary=\", \"{:.4f}\".format(acc))\n",
    "duration = time() - startTime\n",
    "print(\"Train Finished takes:\", \"{:.2f}\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "accu_test = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "print(\"Test Accuracy:\", accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictgion_result=sess.run(tf.argmax(pred, 1),\n",
    "                           feed_dict={x: mnist.test.images})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
